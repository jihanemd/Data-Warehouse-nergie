# üìã ANNEXES - Data Warehouse √ânergie France

## Annexe A ‚Äì Pr√©paration de l'environnement technique

### Syst√®me d'exploitation

- **OS Principal :** Windows 10/11
- **Environnement CLI :** PowerShell 5.1+
- **Virtualisation :** Python Virtual Environment (.venv)

### Outils install√©s

#### Langages et Frameworks

| Outil | Version | Usage |
|-------|---------|-------|
| Python | 3.13.9 | Orchestration ETL, scripts |
| Pandas | ‚â•1.5.0 | Manipulation donn√©es |
| NumPy | ‚â•1.23.0 | Calculs num√©riques |
| PyArrow | Latest | S√©rialisation Parquet |
| PyYAML | ‚â•6.0 | Configuration YAML |

#### Bases de donn√©es

| SGBD | Version | Port |
|------|---------|------|
| PostgreSQL | 14+ | 5432 |

#### Backend API

| Framework | Version | Usage |
|-----------|---------|-------|
| Flask | ‚â•3.1.0 | API REST |
| Flask-CORS | ‚â•6.0.0 | CORS support |
| SQLAlchemy | ‚â•1.4.0 | ORM base de donn√©es |
| psycopg2-binary | ‚â•2.9.0 | Driver PostgreSQL |

### √âtapes d'installation pas √† pas

#### 1. Cr√©er et activer l'environnement virtuel

```powershell
cd Data-Warehouse-nergie
python -m venv .venv
.\.venv\Scripts\Activate.ps1  # Windows PowerShell
```

#### 2. Installer les d√©pendances

```bash
pip install -r requirements.txt
```

#### 3. Configurer PostgreSQL

```bash
# Cr√©er la base de donn√©es
psql -U postgres -c "CREATE DATABASE dw_energie_france OWNER postgres;"

# V√©rifier la connexion
psql -h localhost -U postgres -d dw_energie_france -c "SELECT 1;"
```

#### 4. Configurer les fichiers environnement

Cr√©er `.env` :
```
DB_USER=postgres
DB_PASSWORD=jihane
DB_HOST=localhost
DB_PORT=5432
DB_NAME=dw_energie_france
```

#### 5. V√©rifier l'installation

```bash
python run.py  # Lance le pipeline complet
```

---

## Annexe B ‚Äì Jeux de donn√©es

### Description des jeux de donn√©es

#### france_time_series.csv

- **Source :** RTE (R√©seau de Transport d'√âlectricit√©)
- **Lignes :** 50,393
- **Colonnes :** 12
- **Format :** CSV (UTF-8, d√©limiteur virgule)
- **P√©riode :** 2015-2026
- **Granularit√© :** Horaire
- **Contenu :** Production √©lectrique par type (Solar, Wind, Hydro, Thermal, Nuclear)

#### eurostat_electricity_france.csv

- **Source :** Eurostat (Office statistique UE)
- **Lignes :** 417
- **Colonnes :** 8
- **Format :** CSV (UTF-8, d√©limiteur virgule)
- **P√©riode :** 2015-2026
- **Granularit√© :** Mensuelle
- **Contenu :** Donn√©es statistiques agr√©g√©es production ENR

#### renewable_power_plants_FR.csv

- **Source :** Open Data R√©seaux √ânergies
- **Lignes :** 9,744
- **Colonnes :** 15
- **Format :** CSV (UTF-8, d√©limiteur virgule)
- **P√©riode :** Donn√©es statiques (snapshot courant)
- **Granularit√© :** Par installation
- **Contenu :** Registre ENR avec g√©olocalisation, capacit√©, technologie

### Formats et structure

#### Extrait france_time_series.csv

```csv
date,Solar,Wind Onshore,Hydro,Thermal,Nuclear,Load
2015-01-01 00:00,0,6789,4321,5432,10234,23456
2015-01-01 01:00,0,6543,4210,5321,10123,22345
2015-01-01 02:00,0,6234,4098,5210,10012,21234
```

#### Extrait renewable_power_plants_FR.csv

```csv
plant_id,plant_name,technology,capacity_mw,region,commissioning_date,latitude,longitude,postcode
1,Installation A,Photovoltaic,50.5,√éle-de-France,2020-03-15,48.8566,2.3522,75001
2,Installation B,Wind Onshore,45.3,Bretagne,2018-06-20,48.3895,4.4867,56000
3,Installation C,Hydro,120.8,Auvergne-Rh√¥ne-Alpes,2015-09-10,45.5017,3.8768,63000
```

---

## Annexe C ‚Äì Impl√©mentation de l'int√©gration

### Pipeline d'int√©gration d√©taill√©

#### √âtape 1 : BRONZE (Ingestion)

**Fichier :** `src/jobs/01_bronze_ingest_pandas.py`

```python
def run_bronze_ingestion_pandas():
    """Ing√®re CSV bruts en Parquet sans transformation"""
    
    config = load_config("conf/config.yaml")
    landing_path = config['paths']['landing']
    bronze_path = config['paths']['bronze']
    
    for source in config['sources']:
        # Lire CSV brut
        df = pd.read_csv(
            os.path.join(landing_path, source['file']),
            dtype=str  # Garder comme string (RAW)
        )
        
        # Ajouter colonnes syst√®me
        df['_source_file'] = source['file']
        df['_ingest_ts'] = pd.Timestamp.now()
        
        # √âcrire en Parquet
        df.to_parquet(
            os.path.join(bronze_path, source['name']),
            engine='pyarrow'
        )
        
        print(f"‚úÖ {len(df):,} lignes ing√©r√©es")
```

**R√©sultats :**
- 61,554 lignes ing√©r√©es
- Format : Parquet
- Localisation : `data/warehouse/bronze/`

#### √âtape 2 : SILVER (Nettoyage)

**Fichier :** `src/jobs/02_silver_clean.py`

```python
def run_silver_clean():
    """Nettoie et valide les donn√©es"""
    
    for source in sources:
        df = pd.read_parquet(bronze_path)
        
        # Nettoyage
        df.dropna(inplace=True)  # Supprimer manquants
        df.drop_duplicates(inplace=True)  # Supprimer doublons
        
        # Validation
        invalid_rows = df[df['production_mw'].astype(float) < 0]
        df = df[df['production_mw'].astype(float) >= 0]
        
        # Standardisation formats
        df['date'] = pd.to_datetime(df['date'])
        
        # √âcrire Silver
        df.to_parquet(silver_path, engine='pyarrow')
        
        print(f"‚úÖ {len(df):,} lignes nettoy√©es")
```

**R√©sultats :**
- 61,554 lignes nettoy√©es
- 0 rejets (100% qualit√©)
- Format : Parquet
- Localisation : `data/warehouse/silver/`

#### √âtape 3 : GOLD (Star Schema)

**Fichier :** `src/jobs/03_gold_dwh.py`

```python
def run_gold_dwh():
    """Cr√©e Star Schema optimis√© pour analytics"""
    
    # Charger data Silver
    df = pd.read_parquet(silver_path)
    
    # Cr√©er dimensions
    dim_date = df[['date']].drop_duplicates().reset_index(drop=True)
    dim_date['date_id'] = range(1, len(dim_date) + 1)
    
    dim_energy_type = pd.DataFrame({
        'energy_type_id': [1, 2, 3, 4, 5],
        'name': ['Solar', 'Wind', 'Hydro', 'Load', 'Other'],
        'category': ['Renewable', 'Renewable', 'Renewable', 'Consumption', 'Other']
    })
    
    # Cr√©er faits
    fact_production = df.merge(dim_date, on='date')
    fact_production = fact_production[['date_id', 'energy_type_id', 'production_mw', 'min_mw', 'max_mw', 'avg_mw']]
    
    # √âcrire Gold
    dim_date.to_parquet(gold_path + '/dim_date')
    dim_energy_type.to_parquet(gold_path + '/dim_energy_type')
    fact_production.to_parquet(gold_path + '/fact_energy_production')
    
    print(f"‚úÖ {len(fact_production):,} lignes analytiques")
```

**R√©sultats :**
- 20,488 lignes analytiques
- 6 tables (4 dim + 2 fact)
- Format : Parquet
- Localisation : `data/warehouse/gold/`

#### √âtape 4 : PostgreSQL

**Fichier :** `reload_postgres.py`

```python
def load_gold_to_postgres():
    """Charge Star Schema en PostgreSQL"""
    
    engine = create_engine(
        'postgresql://postgres:jihane@localhost/dw_energie_france'
    )
    
    tables = [
        'dim_date', 'dim_energy_type', 'dim_location', 'dim_plant',
        'fact_energy_production', 'fact_renewable_capacity'
    ]
    
    for table_name in tables:
        df = pd.read_parquet(f'data/warehouse/gold/{table_name}')
        df.to_sql(
            table_name, 
            engine, 
            schema='gold', 
            if_exists='replace',
            index=False
        )
        print(f"‚úÖ {table_name}: {len(df):,} lignes charg√©es")
```

### Orchestration compl√®te

**Fichier :** `run.py`

```python
def main():
    print("üü§ BRONZE - Ingestion brute")
    run_bronze_ingestion_pandas()
    
    print("üü£ SILVER - Nettoyage et validation")
    run_silver_clean()
    
    print("üü° GOLD - Star Schema")
    run_gold_dwh()
    
    print("üóÑÔ∏è  POSTGRESQL - Chargement base relationnelle")
    load_gold_to_postgres()
    
    print("‚úÖ Pipeline complet (~8 secondes)")

if __name__ == "__main__":
    main()
```

---

## Annexe F ‚Äì Guide de reproduction

### √âtapes d'installation

#### Pr√©requis

- Windows 10/11 ou Linux
- Python 3.10+
- PostgreSQL 12+
- Git

#### Installation d√©taill√©e

##### 1. Cloner le projet

```bash
git clone https://github.com/jihanemd/Data-Warehouse-nergie.git
cd Data-Warehouse-nergie
```

##### 2. Cr√©er l'environnement virtuel

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1  # Windows
# ou pour Linux/Mac: source .venv/bin/activate
```

##### 3. Installer les d√©pendances

```bash
pip install -r requirements.txt
```

##### 4. Configurer PostgreSQL

```bash
# Cr√©er la base de donn√©es
psql -U postgres -c "CREATE DATABASE dw_energie_france;"

# V√©rifier la connexion
psql -h localhost -U postgres -d dw_energie_france -c "SELECT 1;"
```

##### 5. Cr√©er le fichier .env

```bash
# .env
DB_USER=postgres
DB_PASSWORD=jihane
DB_HOST=localhost
DB_NAME=dw_energie_france
```

##### 6. Lancer le pipeline

```bash
python run.py
```

### Ordre d'ex√©cution

| Ordre | Composant | Fichier | Dur√©e |
|-------|-----------|---------|-------|
| 1 | Bronze Ingestion | `01_bronze_ingest_pandas.py` | ~2s |
| 2 | Silver Clean | `02_silver_clean.py` | ~1.5s |
| 3 | Gold DWH | `03_gold_dwh.py` | ~2.5s |
| 4 | PostgreSQL Load | `reload_postgres.py` | ~2s |
| 5 | API Backend | `backend_api.py` | Async |
| 6 | Dashboard | `dashboard/index.html` | Async |

### R√©sultats attendus

#### Apr√®s √©tape 1 (Bronze)

```
‚úÖ 61,554 lignes ing√©r√©es
üìÇ data/warehouse/bronze/
   ‚îú‚îÄ‚îÄ france_time_series/data.parquet
   ‚îú‚îÄ‚îÄ eurostat_electricity_france/data.parquet
   ‚îú‚îÄ‚îÄ renewable_power_plants_FR/data.parquet
   ‚îî‚îÄ‚îÄ time_series_60min_sample/data.parquet
```

#### Apr√®s √©tape 2 (Silver)

```
‚úÖ 61,554 lignes nettoy√©es (100% qualit√©)
üìÇ data/warehouse/silver/
   ‚îî‚îÄ‚îÄ (m√™mes fichiers, qualit√© garantie)
```

#### Apr√®s √©tape 3 (Gold)

```
‚úÖ 20,488 lignes analytiques
üìÇ data/warehouse/gold/
   ‚îú‚îÄ‚îÄ dim_date/data.parquet
   ‚îú‚îÄ‚îÄ dim_energy_type/data.parquet
   ‚îú‚îÄ‚îÄ dim_location/data.parquet
   ‚îú‚îÄ‚îÄ dim_plant/data.parquet
   ‚îú‚îÄ‚îÄ fact_energy_production/data.parquet
   ‚îî‚îÄ‚îÄ fact_renewable_capacity/data.parquet
```

#### Apr√®s √©tape 4 (PostgreSQL)

```sql
‚úÖ 6 tables charg√©es en sch√©ma gold

SELECT COUNT(*) FROM gold.fact_energy_production;
-- Result: 6,301 lignes

SELECT COUNT(*) FROM gold.dim_date;
-- Result: 4,383 lignes
```

### V√©rification finale

```bash
# Tester la connexion PostgreSQL
python -c "import sqlalchemy as sa; engine = sa.create_engine('postgresql://postgres:jihane@localhost/dw_energie_france'); print('‚úÖ Connexion OK')"

# Lancer l'API
python backend_api.py  # http://localhost:5000/health

# Lancer le Dashboard
cd dashboard && python -m http.server 8000
# http://localhost:8000/index.html
```

### V√©rifications de performance

#### Timing attendu

- Bronze : 2 secondes
- Silver : 1.5 secondes
- Gold : 2.5 secondes
- PostgreSQL : 2 secondes
- **Total : ~8 secondes**

#### Qualit√© de donn√©es

- Bronze : 61,554 lignes ing√©r√©es
- Silver : 61,554 lignes (0% rejet)
- Gold : 20,488 lignes (67% r√©duction = normal)
- PostgreSQL : 20,488 lignes charg√©es

#### V√©rifications SQL

```sql
-- V√©rifier le sch√©ma
SELECT table_name FROM information_schema.tables 
WHERE table_schema = 'gold'
ORDER BY table_name;

-- Compter les lignes
SELECT 
    'dim_date' as table_name, COUNT(*) as cnt FROM gold.dim_date
UNION ALL
SELECT 'dim_energy_type', COUNT(*) FROM gold.dim_energy_type
UNION ALL
SELECT 'dim_location', COUNT(*) FROM gold.dim_location
UNION ALL
SELECT 'dim_plant', COUNT(*) FROM gold.dim_plant
UNION ALL
SELECT 'fact_energy_production', COUNT(*) FROM gold.fact_energy_production
UNION ALL
SELECT 'fact_renewable_capacity', COUNT(*) FROM gold.fact_renewable_capacity;

-- V√©rifier int√©grit√© des cl√©s √©trang√®res
SELECT COUNT(*) FROM gold.fact_energy_production 
WHERE date_id NOT IN (SELECT date_id FROM gold.dim_date);
-- Should return 0 (int√©grit√© OK)
```

### D√©pannage

#### Erreur: PostgreSQL connection refused

```bash
# V√©rifier que PostgreSQL est lanc√©
pg_isready -h localhost -p 5432

# V√©rifier les credentials dans .env
cat .env
```

#### Erreur: CSV file not found

```bash
# V√©rifier que les fichiers existent
ls data/landing/
# Must show: france_time_series.csv, eurostat_electricity_france.csv, etc.
```

#### Erreur: Module not found

```bash
# R√©installer les d√©pendances
pip install -r requirements.txt --upgrade
```

#### Erreur: Port already in use

```bash
# Pour API (port 5000)
netstat -ano | findstr 5000
taskkill /PID <PID> /F

# Pour Dashboard (port 8000)
netstat -ano | findstr 8000
taskkill /PID <PID> /F
```

---

## R√©sum√© des annexes

‚úÖ **Annexe A :** Configuration technique compl√®te  
‚úÖ **Annexe B :** Description et √©chantillons des sources de donn√©es  
‚úÖ **Annexe C :** Impl√©mentation d√©taill√©e du pipeline ETL  
‚úÖ **Annexe F :** Guide complet de reproduction  

**Date :** 11 Janvier 2026  
**Status :** Production Ready ‚úÖ
